{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection of Offensive Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is to apply Natural Language Processing in data obtained from the web. In this case, we will do web scrapping to Twitter to get the data. We will modify them and obtain a cleaned data, to finally make some predictive models to know if a specific tweet is an insult or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to connect to connect to Twitter to maniputale the data. Tweepy is a package that makes it easier to use the twitter streaming api by handling authentication, make the connection and then we will be able to extract the data we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    " \n",
    "consumer_key = 'XXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "consumer_secret = 'XXXXXXXXXXXXXXXXXXXXXXX'\n",
    "access_token = 'XXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "access_secret = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    " \n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    " \n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#EnVivo Aula Magna XXII: Inteligencia Artificial, Big Data, Machine Learning y Rob√≥tica. #AulaMagnaPUCP https://t.co/Z0L15SvNle\n",
      "RT @Rudgrcom: Birthdays are best with friends, and cake. Lots of cake. Happy celebrating @davidguetta! üéÇüéÅü•Ç https://t.co/ePJZq4a1ih https://‚Ä¶\n",
      "S√∫bele a tu mi√©rcoles con buena m√∫sica üîâüé∂ crea tu Playlist favorito en @spotify con todos mis temas m√°s recientes c‚Ä¶ https://t.co/OlTRI3FuHu\n",
      "RT @NRJhitmusiconly: Les patrons sont au rendez vous !!!! üòÄ\n",
      "David GUETTA et  Martin GARRIX raflent tout sur leur passage.\n",
      "Ils remportent le‚Ä¶\n",
      "J'adore cette journ√©e üôèüôèüôè\n",
      "https://t.co/mxew8SWh1s\n",
      "RT @martinsolveig: Happy birthday David ‚úåÔ∏è https://t.co/RkR4m4wXJO https://t.co/flI9SFntr4\n",
      "RT @KungsMusic: Happy birthday boss @davidguetta and congrats for this amazing year. True legend üôèüèª https://t.co/f9HuzR2oyz\n",
      "\"Adi√≥s a la incapacidad civil de las personas con discapacidad mental\". Escribe Renata Bregaglio, profesora del Dep‚Ä¶ https://t.co/eEJncFR7Aq\n",
      "I just can‚Äôt wait! ü§µüèªüë∞üèª üéªüé∂üçæ Love U bae! @jo__lyn ‚ù§Ô∏è https://t.co/Ze9r72V56H\n",
      "I just can‚Äôt wait! ü§µüèªüë∞üèª üéªüé∂üçæ Love U bae! jo__lyn ‚ù§Ô∏è https://t.co/en8SK4zwkX\n"
     ]
    }
   ],
   "source": [
    "for status in tweepy.Cursor(api.home_timeline).items(10):\n",
    "    # Process a single status\n",
    "    print(status.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will skip the part of obtaining the different tweets. Also, we have to decide if each tweet in our dataset is an insult or not. We need to define it using a flag, this can be made creating a new column in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use nltk to make the cleaning process of our data. First, we will use tokenization to separate each word. After that,  After that, we will make shorter our words. Between lemmatization and stemming, we prefer to use stemming because it is simpler, smaller and usually faster. Then, we will get rid of the words that are not important, such as ‚Äúthe‚Äù, ‚Äúa‚Äù, ‚Äúan‚Äù, ‚Äúin‚Äù. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()   #to lemmatize\n",
    "ps = PorterStemmer()    #stemming\n",
    "tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()  \n",
    "stop_words = set(stopwords.words('english'))       \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the instances, we will make use of them. We will clean all tweets making a for-loop. Have in mind we will make use of regular expressions, which are special sequences of characters that help to match or find other strings or sets of strings. In this case, this will help us to eliminate some characters we find useless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['``',\n",
       "  'realli',\n",
       "  \"n't\",\n",
       "  'understand',\n",
       "  'point',\n",
       "  '.',\n",
       "  'It',\n",
       "  'seem',\n",
       "  'mix',\n",
       "  'appl',\n",
       "  'orang',\n",
       "  '.',\n",
       "  \"''\"],\n",
       " 0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=[]\n",
    "comments=[]\n",
    "num=0\n",
    "\n",
    "for comment in tweets: \n",
    "    word_tokens=word_tokenize(comment)   #tokenizing the comment\n",
    "    filtered_sentence = []\n",
    "    \n",
    "    for w in word_tokens:           #each word of the comment\n",
    "        w1=ps.stem(w)                  #stemming the word\n",
    "        #w1=lemmatizer.lemmatize(w1)    #its not necessary lemmatizing\n",
    "\n",
    "        if w1 not in stop_words:        #if its not a stop words, enter to the if-statement\n",
    "            w2 = re.sub(r'(\\\\xa0)', r' ', w1)     #erasing the emoticons, or dirty characters\n",
    "            w3 = re.sub(r'(\\xa0)', r' ', w2)\n",
    "            w3=w3.replace(u'\\\\n', u' ')\n",
    "            w3=w3.replace(u'\\\\xc2', u' ')\n",
    "            w3=w3.replace(u'\\\\u', u' ')\n",
    "            w3=w3.replace(u'\\\\x', u' ')\n",
    "            w4 = re.sub(r'(\\xc2)', r' ', w3)\n",
    "            w5 = re.sub(r'[^s](\\n)', r' ', w4)  \n",
    "            w6 = re.sub(r'(\\n)', r' ', w5)\n",
    "            w7 = re.sub(r'(\\xec)', r' ', w6)\n",
    "            w8 = re.sub(r'($)', r' ', w7)             #erasing dollars\n",
    "            w9 = re.sub(r'[^\\s]+@[^\\s]+', r'', w8)     #erasing emails\n",
    "            w10 = re.sub(r'@[^\\s]+', r' ', w9)            #erasing userId of people i.e. @alex01 \n",
    "            w11 = re.sub(r'(http|https)://[^\\s]*', r'', w10)    #erasing websites\n",
    "            w11=w11.replace(u'\\\\', u' ')\n",
    "            word1 = re.sub(r'[0-9]+', r'', w11)        #erasing numbers\n",
    "            \n",
    "            \n",
    "            word_tokens_Aux=word_tokenize(word1)   \n",
    "            \n",
    "            if len(word_tokens_Aux)!=1:              \n",
    "                for w_Aux in word_tokens_Aux:\n",
    "                    w1_Aux=ps.stem(w_Aux)             \n",
    "                    if w1_Aux not in stop_words:\n",
    "                        filtered_sentence.append(w1_Aux.lower())      \n",
    "                        features.append(w1_Aux.lower())            \n",
    "            else:\n",
    "                filtered_sentence.append(word_tokens_Aux[0]) \n",
    "                features.append(word_tokens_Aux[0])        \n",
    "                \n",
    "    \n",
    "    comments.append( [filtered_sentence, dataset.iloc[num,0]] )   \n",
    "    num=num+1\n",
    "    \n",
    "comments[1]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see one tweet after the cleaning process. It looks pretty good in comparison how we found it at first. Now, we will use only the first 10,000 words to do the predictive analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(features)\n",
    "main_words=all_words.most_common(10000)\n",
    "word_features=[]\n",
    "for w in main_words:\n",
    "    w_list=list(w)\n",
    "    word_features.append(w[0])\n",
    "#word_features\n",
    "\n",
    "word_features = list(all_words.keys())[:10]\n",
    "#word_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create our defined format to put in our machine learning model. In this case, our table containing all the data will be defined in featuresets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets=[]\n",
    "\n",
    "for comment1 in comments:\n",
    "    features_Aux={}\n",
    "    for w in word_features:\n",
    "        \n",
    "        features_Aux[w]=w in comment1[0]\n",
    "    featuresets.append((features_Aux,comment1[1]))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we will split our dataset in training set and test set. After that, we will use some machine learning models (such as Naive Bayes, Bernoulli, Logistic Regression, SGD, SVC) to classify the tweets. Finally, we will score our models with the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Naive Bayes Algo accuracy percent: 72.26502311248075\n",
      "BernoulliNB_classifier accuracy percent: 72.26502311248075\n",
      "LogisticRegression_classifier accuracy percent: 72.26502311248075\n",
      "SGDClassifier_classifier accuracy percent: 72.31638418079096\n",
      "SVC_classifier accuracy percent: 72.47046738572163\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "training_set = featuresets[:2000]\n",
    "testing_set =  featuresets[2000:]\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Original Naive Bayes Algo accuracy percent:\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "\n",
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(training_set)\n",
    "print(\"BernoulliNB_classifier accuracy percent:\", (nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100)\n",
    "\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)\n",
    "\n",
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(training_set)\n",
    "print(\"SGDClassifier_classifier accuracy percent:\", (nltk.classify.accuracy(SGDClassifier_classifier, testing_set))*100)\n",
    "\n",
    "SVC_classifier = SklearnClassifier(SVC())\n",
    "SVC_classifier.train(training_set)\n",
    "print(\"SVC_classifier accuracy percent:\", (nltk.classify.accuracy(SVC_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC Classifier has the greatest accuracy, that's why we choose this model to go forward and deploy our model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
